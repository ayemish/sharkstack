{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVY6ZHqX0aoK7a8RqIrkqx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayemish/sharkstack/blob/main/task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip uninstall -y pinecone-client pinecone\n",
        "!pip install pinecone langchain langchain-pinecone langchain-community langchain-text-splitters sentence-transformers pypdf\n"
      ],
      "metadata": {
        "id": "0lE5beb8ij9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain_pinecone import PineconeVectorStore\n"
      ],
      "metadata": {
        "id": "klmx8SenkQmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_6TDdRa_Eu5mZiJM1TX1cjTstQ9Ku8NxvYEFjXaFzfVkt63tgud7oJkqx2aHpgKJcKjGQSp\"\n"
      ],
      "metadata": {
        "id": "xq-oqrx1kUhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"book.pdf\")\n",
        "documents = loader.load()\n",
        "print(f\"Total pages loaded: {len(documents)}\")\n"
      ],
      "metadata": {
        "id": "FkIuNBkRk7j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\", \" \", \"\"]\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Total chunks: {len(chunks)}\")\n"
      ],
      "metadata": {
        "id": "EK_kOyqVlR2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "ei_wK9PIlWZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "\n",
        "index_name = \"book-rag-chatbot\"\n",
        "\n",
        "# create index if not exists\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,   # embedding size for MiniLM-L6-v2\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "\n",
        "# wait a bit before proceeding (index takes some seconds to initialize)\n"
      ],
      "metadata": {
        "id": "Kzc4mtG-lZfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    index_name=index_name\n",
        ")\n"
      ],
      "metadata": {
        "id": "zlDHkhvdljah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# load a small QA model\n",
        "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
        "\n",
        "def rag_chatbot(query):\n",
        "    # search in Pinecone\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "    context = \" \".join([doc.page_content for doc in results])\n",
        "\n",
        "    # instruction: only answer from context\n",
        "    prompt = f\"\"\"Answer the question using ONLY the following context:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    If the answer is not in the context, say: \"I could not find it.\"\n",
        "    \"\"\"\n",
        "\n",
        "    output = qa_model(prompt, max_length=256, do_sample=False)\n",
        "    return output[0]['generated_text']\n",
        "\n",
        "# Example\n",
        "print(rag_chatbot(\"What is the main theme of chapter 5?\"))\n"
      ],
      "metadata": {
        "id": "flv1rpXSlklT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chatbot(\"Summarize chapter 2.\")\n",
        "rag_chatbot(\"Who are the main characters?\")\n",
        "rag_chatbot(\"What happens at the end of the book?\")\n"
      ],
      "metadata": {
        "id": "Xr-jyH12nl7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ðŸ“š Book Chatbot\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(placeholder=\"Ask a question about your book...\")\n",
        "    clear = gr.Button(\"Clear\")\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        bot_reply = rag_chatbot(message)   # use your existing function\n",
        "        chat_history.append((message, bot_reply))\n",
        "        return \"\", chat_history\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "2oV-PmNuoL_R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}